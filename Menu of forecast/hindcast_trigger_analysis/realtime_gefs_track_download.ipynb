{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, csv, sys \n",
    "import scipy\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_rounder(t):\n",
    "    # Rounds to nearest hour by adding a timedelta hour if minute >= 30\n",
    "    return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour) - timedelta(hours=t.hour-6))\n",
    "\n",
    "def xml2csvAll(filename,save_dir):\n",
    "    print(f\"{filename}\")\n",
    "    dateObj=os.path.basename(filename).split('_')[4]  \n",
    "    forecastTimeObj=datetime.strptime(dateObj,\"%Y%m%d%H%M%S\")\n",
    "    dfs=[]\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(filename)\n",
    "    except ET.ParseError:\n",
    "        print(\"Error with file, skipping\")\n",
    "        return\n",
    "    root = tree.getroot()\n",
    "\n",
    "    prod_center=root.find('header/productionCenter').text\n",
    "    model_name=root.find('header/generatingApplication/model/name').text\n",
    "    baseTime=root.find('header/baseTime').text\n",
    "    print(model_name)\n",
    "\n",
    "    ## Create one dictonary for each time point, and append it to a list\n",
    "    for members in root.findall('data'):\n",
    "        mtype=members.get('type')\n",
    "        if mtype not in ['forecast', 'ensembleForecast']:\n",
    "            continue\n",
    "        for members2 in members.findall('disturbance'):\n",
    "            \n",
    "            cyclone_ID=members2.get('ID')      \n",
    "            print(f\"Found typhoon {cyclone_ID}\")\n",
    "            cyclone_name = [name.text.lower().strip() for name in members2.findall('cycloneName')]\n",
    "            basin_name = [name.text.lower().strip() for name in members2.findall('basin')]\n",
    "        \n",
    "            if 'wp' not in basin_name:\n",
    "                continue\n",
    "            cyclone_name = cyclone_name[0].lower()\n",
    "            print(f\"Found typhoon {cyclone_name}\")\n",
    "            for members3 in members2.findall('fix'):\n",
    "                tem_dic = {}\n",
    "                time = [name.text for name in members3.findall('validTime')]\n",
    "                #tem_dic['mtype']=[mtype]\n",
    "                tem_dic['model_name']=[model_name]\n",
    "                tem_dic['cyclone_ID']=[cyclone_ID]\n",
    "                tem_dic['forecastTime']=[forecastTimeObj]\n",
    "                #tem_dic['product']=[re.sub('\\s+',' ',prod_center).strip().lower()]\n",
    "                #tem_dic['cyc_number'] = [name.text for name in members2.findall('cycloneNumber')]\n",
    "                tem_dic['ensemble']=[members.get('member')]\n",
    "                tem_dic['speed'] = [name.text for name in members3.findall('cycloneData/maximumWind/speed')]\n",
    "                tem_dic['pressure'] = [name.text for name in members3.findall('cycloneData/minimumPressure/pressure')]              \n",
    "                tem_dic['time'] = ['/'.join(time[0].split('T')[0].split('-'))+', '+time[0].split('T')[1][:-1]]\n",
    "                tem_dic['lat'] = [name.text for name in members3.findall('latitude')]\n",
    "                tem_dic['lon']= [name.text for name in members3.findall('longitude')]\n",
    "                tem_dic['lead_time']=[members3.get('hour')]\n",
    "                #tem_dic['forecast_time'] = ['/'.join(baseTime.split('T')[0].split('-'))+', '+baseTime.split('T')[1][:-1]]\n",
    "                tem_dic1 = dict( [(k,''.join(str(e).lower().strip() for e in v)) for k,v in tem_dic.items()])\n",
    "                # Save to CSV\n",
    "                dfs.append(pd.DataFrame(tem_dic1, index=[0]))\n",
    "    fn=os.path.basename(filename)\n",
    "    outfile = save_dir / f\"csv/{fn}.csv\"\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    concatenated_df.to_csv(outfile, mode='a', header=not os.path.exists(outfile), index=False)\n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "data_folderxml = 'data/xml/'\n",
    "data_foldercsv = 'data/csv/'\n",
    "results_folder = 'results/'\n",
    "figures_folder = 'figures/'\n",
    "\n",
    "delete_previous_results_files = 'y'\n",
    "save_csv_files = 'y'  # one per model\n",
    "\n",
    "# Create folders\n",
    "\n",
    "for folder_name in [data_folderxml, data_foldercsv,results_folder, figures_folder]:\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem_dir='data/'\n",
    "\n",
    " \n",
    "save_dir=os.path.join(tem_dir,'xml')\n",
    "save_dir = Path(save_dir)\n",
    "\n",
    "email = 'ateklesadik@redcross.nl'\n",
    "pswd = 'yDrasYcB'\n",
    "\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "\n",
    "ret = requests.post(login_url, data=values)\n",
    "\n",
    "\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "\n",
    "ret = requests.post(login_url, data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)\n",
    "\n",
    "dspath = 'https://rda.ucar.edu/data/ds330.3/' #https://rda.ucar.edu/datasets/ds330.3/\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\n",
    "tem_dir=os.getcwd()\n",
    "\n",
    "save_dir=os.path.join(tem_dir,'data')\n",
    "save_dir = Path(save_dir)\n",
    "\n",
    "current_date = datetime.utcnow()\n",
    "current_date=hour_rounder(current_date)\n",
    "end_date = current_date\n",
    "start_date =current_date - timedelta(hours=12)\n",
    "\n",
    "date_list = rrule.rrule(rrule.HOURLY, \n",
    "                        dtstart=start_date, \n",
    "                        until=end_date,\n",
    "                        interval=6)\n",
    "\n",
    " \n",
    "forecaster='kwbc'\n",
    "modelList={'CENS':'esttr',\n",
    "           'CMC':'sttr',\n",
    "           'GEFS':'esttr',\n",
    "           'GFS':'sttr'}\n",
    "for key, value in modelList.items():\n",
    "    model=key\n",
    "    modelcode=value\n",
    "    for date in date_list:\n",
    "        ymd = date.strftime(\"%Y%m%d\")\n",
    "        ymdhms = date.strftime(\"%Y%m%d%H%M%S\")\n",
    "        server = \"test\" if date < datetime(2008, 8, 1) else \"prod\"\n",
    "        file = f'{forecaster}/{date.year}/{ymd}/z_tigge_c_{forecaster}_{ymdhms}_{model}_glob_{server}_{modelcode}_glo.xml'\n",
    "        filename = 'https://data.rda.ucar.edu/ds330.3/' + file\n",
    "        outfile = save_dir / \"xml\" / os.path.basename(filename)\n",
    "        # Don't download if exists already\n",
    "        if outfile.exists():\n",
    "            if verbose:\n",
    "                print(f'{file} already exists')\n",
    "            continue\n",
    "        req = requests.get(filename, cookies = ret.cookies, allow_redirects=True)\n",
    "        if req.status_code != 200:\n",
    "            if verbose:\n",
    "                print(f'{file} invalid URL')\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f'{file} downloading')\n",
    "        open(outfile, 'wb').write(req.content)\n",
    "\n",
    "\n",
    "# Get list of filenames\n",
    "filename_list = sorted(list(Path(save_dir / \"xml\").glob('*.xml')))\n",
    "for filename in filename_list[-4:]:\n",
    "    xml2csvAll(filename,save_dir)\n",
    "\n",
    "filename_list = sorted(list(Path(save_dir / \"csv\").glob('*.csv')))\n",
    "dfs=[pd.read_csv(f) for f in filename_list]\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
